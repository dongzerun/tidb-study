Week 2 TiDB 基准测试
### 机器环境
阿里云机器，一共申请了 3 台 `ecs.c6.xlarge` 用于 PD, 3 台 `ecs.hfg6.2xlarge` 用于 tikv, 1 台 `ecs.c6.2xlarge` 用于 tidb, 1 台 `ecs.hfg6.2xlarge` 用于 benchmark client
|   | cpu  | memory  |  disk | ip  |
|---|---|---|---|---|
|  pd1 | 4  |  8G |  20G | 172.24.213.46  |
| pd2  |  4 |  8G | 20G  | 172.24.213.45  |
|  pd3 |  4 |  8G | 20G  | 172.24.213.47  |
| tikv1  |  8 |  32G | 100G SSD |  172.24.213.50 |
|  tikv1 | 8  | 32G  |  100G SSD | 172.24.213.48  |
|  tikv1 |  8 | 32G  | 100G SSD  | 172.24.213.49  |
|  tidb |  8 | 16G  | 20G  | 172.24.213.51  |
|  bench |   |  |   |   |

系统均为 ubuntu 18.04
```shell
root@tidb:~# uname -a
Linux tidb 4.15.0-111-generic #112-Ubuntu SMP Thu Jul 9 20:32:34 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
```
### 搭建服务
#### 1.TiKV 挂载 SSD
阿里云申请了各 100G 的 SSD, 需要格式化成 ext4 并挂载，第一步格式化文件系统
```shell
root@tikv001:~# fdisk -l
Disk /dev/vda: 20 GiB, 21474836480 bytes, 41943040 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0xc0f4f2f1

Device     Boot Start      End  Sectors Size Id Type
/dev/vda1  *     2048 41940991 41938944  20G 83 Linux


Disk /dev/vdb: 100 GiB, 107374182400 bytes, 209715200 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
root@tikv001:~#
root@tikv001:~#
root@tikv001:~# parted -s -a optimal /dev/vdb  mklabel gpt -- mkpart primary ext4 1 -1
root@tikv001:~# mkfs.ext4 /dev/vdb
mke2fs 1.44.1 (24-Mar-2018)
Found a gpt partition table in /dev/vdb
Proceed anyway? (y,N) y
Creating filesystem with 26214400 4k blocks and 6553600 inodes
Filesystem UUID: d7f9666d-4e47-4c2d-b9c2-d44cbc34be75
Superblock backups stored on blocks:
    32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208,
    4096000, 7962624, 11239424, 20480000, 23887872

Allocating group tables: done
Writing inode tables: done
Creating journal (131072 blocks): done
Writing superblocks and filesystem accounting information: done
```
第二步，挂载到 /tidb-data, 三台 tikv 机器重复这两步
```shell
root@tikv001:~# lsblk -f
NAME   FSTYPE LABEL UUID                                 MOUNTPOINT
vda
└─vda1 ext4   /     3d375280-1ab5-4559-a895-845bc4cb23c0 /
vdb    ext4         d7f9666d-4e47-4c2d-b9c2-d44cbc34be7
root@tikv001:~# echo "UUID=d7f9666d-4e47-4c2d-b9c2-d44cbc34be75 /tidb-data ext4 defaults,nodelalloc,noatime 0 2" >> /etc/fstab
root@tikv001:~# cat /etc/fstab
# /etc/fstab: static file system information.
#
# Use 'blkid' to print the universally unique identifier for a
# device; this may be used with UUID= as a more robust way to name devices
# that works even if disks are added and removed. See fstab(5).
#
# <file system> <mount point>   <type>  <options>       <dump>  <pass>
# / was on /dev/vda1 during installation
UUID=3d375280-1ab5-4559-a895-845bc4cb23c0 /               ext4    errors=remount-ro 0       1
/dev/fd0        /media/floppy0  auto    rw,user,noauto,exec,utf8 0       0
UUID=d7f9666d-4e47-4c2d-b9c2-d44cbc34be75 /tidb-data ext4 defaults,nodelalloc,noatime 0 2
root@tikv001:~# mkdir /tidb-data
root@tikv001:~# mount -a
```
查看挂载情况
```shell
root@tikv001:~# mount -t ext4
/dev/vda1 on / type ext4 (rw,relatime,errors=remount-ro,data=ordered)
/dev/vdb on /tidb-data type ext4 (rw,noatime,nodelalloc,data=ordered)
root@tikv001:~# df -h
Filesystem      Size  Used Avail Use% Mounted on
udev             16G     0   16G   0% /dev
tmpfs           3.1G  4.0M  3.1G   1% /run
/dev/vda1        20G  3.2G   16G  17% /
tmpfs            16G     0   16G   0% /dev/shm
tmpfs           5.0M     0  5.0M   0% /run/lock
tmpfs            16G     0   16G   0% /sys/fs/cgroup
tmpfs           3.1G     0  3.1G   0% /run/user/0
/dev/vdb         98G   61M   93G   1% /tidb-data
```
#### 2.Tiup 下载安装
```shell
root@tidb:~# curl --proto '=https' --tlsv1.2 -sSf https://tiup-mirrors.pingcap.com/install.sh | sh
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 4422k  100 4422k    0     0  4735k      0 --:--:-- --:--:-- --:--:-- 4730k
WARN: adding root certificate via internet: https://tiup-mirrors.pingcap.com/root.json
You can revoke this by remove /root/.tiup/bin/7b8e153f2e2d0928.root.json
Set mirror to https://tiup-mirrors.pingcap.com success
Detected shell: bash
Shell profile:  /root/.bashrc
/root/.bashrc has been modified to to add tiup to PATH
open a new terminal or source /root/.bashrc to use it
Installed path: /root/.tiup/bin/tiup
===============================================
Have a try:     tiup playground
===============================================
root@tidb:~# source /root/.bashrc
```
#### 3.Tiup 启动集群
从官网下载好 `topology.yaml` 模版，编辑填写正确的 ip 地址
```shell
root@tidb:~# tiup cluster deploy tidb-benchmark nightly ./topology.yaml --user root -p
The component `cluster` is not installed; downloading from repository.
download https://tiup-mirrors.pingcap.com/cluster-v1.0.9-linux-amd64.tar.gz 9.86 MiB / 9.86 MiB 100.00% ? p/s
Starting component `cluster`: /root/.tiup/components/cluster/v1.0.9/tiup-cluster deploy tidb-benchmark nightly ./topology.yaml --user root -p
Please confirm your topology:
tidb Cluster: tidb-benchmark
tidb Version: nightly
Type          Host           Ports        OS/Arch       Directories
----          ----           -----        -------       -----------
pd            172.24.213.46  2379/2380    linux/x86_64  /tidb-deploy/pd-2379,/tidb-data/pd-2379
pd            172.24.213.45  2379/2380    linux/x86_64  /tidb-deploy/pd-2379,/tidb-data/pd-2379
pd            172.24.213.47  2379/2380    linux/x86_64  /tidb-deploy/pd-2379,/tidb-data/pd-2379
tikv          172.24.213.50  20160/20180  linux/x86_64  /tidb-deploy/tikv-20160,/tidb-data/tikv-20160
tikv          172.24.213.48  20160/20180  linux/x86_64  /tidb-deploy/tikv-20160,/tidb-data/tikv-20160
tikv          172.24.213.49  20160/20180  linux/x86_64  /tidb-deploy/tikv-20160,/tidb-data/tikv-20160
tidb          172.24.213.51  4000/10080   linux/x86_64  /tidb-deploy/tidb-4000
prometheus    172.24.213.49  9090         linux/x86_64  /tidb-deploy/prometheus-9090,/tidb-data/prometheus-9090
grafana       172.24.213.48  3000         linux/x86_64  /tidb-deploy/grafana-3000
alertmanager  172.24.213.50  9093/9094    linux/x86_64  /tidb-deploy/alertmanager-9093,/tidb-data/alertmanager-9093
Attention:
    1. If the topology is not what you expected, check your yaml file.
    2. Please confirm there is no port/directory conflicts in same host.
Do you want to continue? [y/N]:  y
+ Generate SSH keys ... Done
+ Download TiDB components
  - Download pd:nightly (linux/amd64) ... Done
  - Download tikv:nightly (linux/amd64) ... Done
  - Download tidb:nightly (linux/amd64) ... Done
  - Download prometheus:nightly (linux/amd64) ... Done
  - Download grafana:nightly (linux/amd64) ... Done
  - Download alertmanager:v0.17.0 (linux/amd64) ... Done
  - Download node_exporter:v0.17.0 (linux/amd64) ... Done
  - Download blackbox_exporter:v0.12.0 (linux/amd64) ... Done
+ Initialize target host environments
  - Prepare 172.24.213.46:22 ... Done
  - Prepare 172.24.213.45:22 ... Done
  - Prepare 172.24.213.47:22 ... Done
  - Prepare 172.24.213.50:22 ... Done
  - Prepare 172.24.213.48:22 ... Done
  - Prepare 172.24.213.49:22 ... Done
  - Prepare 172.24.213.51:22 ... Done
+ Copy files
  - Copy pd -> 172.24.213.46 ... Done
  - Copy pd -> 172.24.213.45 ... Done
  - Copy pd -> 172.24.213.47 ... Done
  - Copy tikv -> 172.24.213.50 ... Done
  - Copy tikv -> 172.24.213.48 ... Done
  - Copy tikv -> 172.24.213.49 ... Done
  - Copy tidb -> 172.24.213.51 ... Done
  - Copy prometheus -> 172.24.213.49 ... Done
  - Copy grafana -> 172.24.213.48 ... Done
  - Copy alertmanager -> 172.24.213.50 ... Done
  - Copy node_exporter -> 172.24.213.49 ... Done
  - Copy node_exporter -> 172.24.213.51 ... Done
  - Copy node_exporter -> 172.24.213.46 ... Done
  - Copy node_exporter -> 172.24.213.45 ... Done
  - Copy node_exporter -> 172.24.213.47 ... Done
  - Copy node_exporter -> 172.24.213.50 ... Done
  - Copy node_exporter -> 172.24.213.48 ... Done
  - Copy blackbox_exporter -> 172.24.213.45 ... Done
  - Copy blackbox_exporter -> 172.24.213.47 ... Done
  - Copy blackbox_exporter -> 172.24.213.50 ... Done
  - Copy blackbox_exporter -> 172.24.213.48 ... Done
  - Copy blackbox_exporter -> 172.24.213.49 ... Done
  - Copy blackbox_exporter -> 172.24.213.51 ... Done
  - Copy blackbox_exporter -> 172.24.213.46 ... Done
+ Check status
Deployed cluster `tidb-benchmark` successfully, you can start the cluster via `tiup cluster start tidb-benchmark`
```
己经安装好 tidb 集群，现在 `tiup cluster start tidb-benchmark` 开始启动
```shell
root@tidb:~# tiup cluster start tidb-benchmark
Starting component `cluster`: /root/.tiup/components/cluster/v1.0.9/tiup-cluster start tidb-benchmark
Starting cluster tidb-benchmark...
+ [ Serial ] - SSHKeySet: privateKey=/root/.tiup/storage/cluster/clusters/tidb-benchmark/ssh/id_rsa, publicKey=/root/.tiup/storage/cluster/clusters/tidb-benchmark/ssh/id_rsa.pub
+ [Parallel] - UserSSH: user=tidb, host=172.24.213.50
+ [Parallel] - UserSSH: user=tidb, host=172.24.213.50
+ [Parallel] - UserSSH: user=tidb, host=172.24.213.49
+ [Parallel] - UserSSH: user=tidb, host=172.24.213.46
+ [Parallel] - UserSSH: user=tidb, host=172.24.213.48
+ [Parallel] - UserSSH: user=tidb, host=172.24.213.49
+ [Parallel] - UserSSH: user=tidb, host=172.24.213.48
+ [Parallel] - UserSSH: user=tidb, host=172.24.213.47
+ [Parallel] - UserSSH: user=tidb, host=172.24.213.45
+ [Parallel] - UserSSH: user=tidb, host=172.24.213.51
+ [ Serial ] - StartCluster
Starting component pd
    Starting instance pd 172.24.213.47:2379
    Starting instance pd 172.24.213.45:2379
    Starting instance pd 172.24.213.46:2379
    Start pd 172.24.213.46:2379 success
    Start pd 172.24.213.45:2379 success
    Start pd 172.24.213.47:2379 success
Starting component node_exporter
    Starting instance 172.24.213.46
    Start 172.24.213.46 success
Starting component blackbox_exporter
    Starting instance 172.24.213.46
    Start 172.24.213.46 success
Starting component node_exporter
    Starting instance 172.24.213.45
    Start 172.24.213.45 success
Starting component blackbox_exporter
    Starting instance 172.24.213.45
    Start 172.24.213.45 success
Starting component node_exporter
    Starting instance 172.24.213.47
    Start 172.24.213.47 success
Starting component blackbox_exporter
    Starting instance 172.24.213.47
    Start 172.24.213.47 success
Starting component tikv
    Starting instance tikv 172.24.213.49:20160
    Starting instance tikv 172.24.213.50:20160
    Starting instance tikv 172.24.213.48:20160
    Start tikv 172.24.213.50:20160 success
    Start tikv 172.24.213.49:20160 success
    Start tikv 172.24.213.48:20160 success
Starting component node_exporter
    Starting instance 172.24.213.50
    Start 172.24.213.50 success
Starting component blackbox_exporter
    Starting instance 172.24.213.50
    Start 172.24.213.50 success
Starting component node_exporter
    Starting instance 172.24.213.48
    Start 172.24.213.48 success
Starting component blackbox_exporter
    Starting instance 172.24.213.48
    Start 172.24.213.48 success
Starting component node_exporter
    Starting instance 172.24.213.49
    Start 172.24.213.49 success
Starting component blackbox_exporter
    Starting instance 172.24.213.49
    Start 172.24.213.49 success
Starting component tidb
    Starting instance tidb 172.24.213.51:4000
    Start tidb 172.24.213.51:4000 success
Starting component node_exporter
    Starting instance 172.24.213.51
    Start 172.24.213.51 success
Starting component blackbox_exporter
    Starting instance 172.24.213.51
    Start 172.24.213.51 success
Starting component prometheus
    Starting instance prometheus 172.24.213.49:9090
    Start prometheus 172.24.213.49:9090 success
Starting component grafana
    Starting instance grafana 172.24.213.48:3000
    Start grafana 172.24.213.48:3000 success
Starting component alertmanager
    Starting instance alertmanager 172.24.213.50:9093
    Start alertmanager 172.24.213.50:9093 success
+ [ Serial ] - UpdateTopology: cluster=tidb-benchmark
Started cluster `tidb-benchmark` successfully
```
测试创建一下 mysql 数据库
```shell
root@tidb:~# tiup cluster display tidb-benchmark
Starting component `cluster`: /root/.tiup/components/cluster/v1.0.9/tiup-cluster display tidb-benchmark
tidb Cluster: tidb-benchmark
tidb Version: nightly
ID                   Role          Host           Ports        OS/Arch       Status  Data Dir                      Deploy Dir
--                   ----          ----           -----        -------       ------  --------                      ----------
172.24.213.50:9093   alertmanager  172.24.213.50  9093/9094    linux/x86_64  Up      /tidb-data/alertmanager-9093  /tidb-deploy/alertmanager-9093
172.24.213.48:3000   grafana       172.24.213.48  3000         linux/x86_64  Up      -                             /tidb-deploy/grafana-3000
172.24.213.45:2379   pd            172.24.213.45  2379/2380    linux/x86_64  Up|L    /tidb-data/pd-2379            /tidb-deploy/pd-2379
172.24.213.46:2379   pd            172.24.213.46  2379/2380    linux/x86_64  Up|UI   /tidb-data/pd-2379            /tidb-deploy/pd-2379
172.24.213.47:2379   pd            172.24.213.47  2379/2380    linux/x86_64  Up      /tidb-data/pd-2379            /tidb-deploy/pd-2379
172.24.213.49:9090   prometheus    172.24.213.49  9090         linux/x86_64  Up      /tidb-data/prometheus-9090    /tidb-deploy/prometheus-9090
172.24.213.51:4000   tidb          172.24.213.51  4000/10080   linux/x86_64  Up      -                             /tidb-deploy/tidb-4000
172.24.213.48:20160  tikv          172.24.213.48  20160/20180  linux/x86_64  Up      /tidb-data/tikv-20160         /tidb-deploy/tikv-20160
172.24.213.49:20160  tikv          172.24.213.49  20160/20180  linux/x86_64  Up      /tidb-data/tikv-20160         /tidb-deploy/tikv-20160
172.24.213.50:20160  tikv          172.24.213.50  20160/20180  linux/x86_64  Up      /tidb-data/tikv-20160         /tidb-deploy/tikv-20160
root@tidb:~# mysql -uroot -h172.24.213.51 -P 4000
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 2
Server version: 5.7.25-TiDB-v4.0.0-beta.2-1016-gf5fa3e7aa TiDB Server (Apache License 2.0) Community Edition, MySQL 5.7 compatible

Copyright (c) 2000, 2020, Oracle and/or its affiliates. All rights reserved.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> use test;
Database changed
mysql> create table a (a int, b int, c int);
Query OK, 0 rows affected (0.07 sec)

mysql> insert into a values (1,1,1);
Query OK, 1 row affected (0.01 sec)

mysql> insert into a values (2,2,2);
Query OK, 1 row affected (0.00 sec)

mysql> select * from a;
+------+------+------+
| a    | b    | c    |
+------+------+------+
|    1 |    1 |    1 |
|    2 |    2 |    2 |
+------+------+------+
2 rows in set (0.00 sec)
```
#### 4.查看 dashboard
打开网址 `http://39.99.45.157:2379/dashboard/#/statement` 查看
![dashboard](./img/dashboard.png)
这里有两个问题，首先阿里云专有网络默认是不把内网端口对外开放的，需要重新设置安全组，把 2379 端口暴露出去，并重启阿里云才能用公网 ip 访问内网 dashboard
### 调整 TiDB TiKV 配置
配置如下，分别是 tidb 和 tikv
### sysbench 压测
#### 1. 安装 sysbench
```shell
root@bench:~# curl -s https://packagecloud.io/install/repositories/akopytov/sysbench/script.deb.sh | sudo bash
Detected operating system as Ubuntu/bionic.
Checking for curl...
Detected curl...
Checking for gpg...
Detected gpg...
Running apt-get update... done.
Installing apt-transport-https... done.
Installing /etc/apt/sources.list.d/akopytov_sysbench.list...done.
Importing packagecloud gpg key... done.
Running apt-get update... done.

The repository is setup! You can now install packages.
root@bench:~# apt install sysbench -y
Reading package lists... Done
Building dependency tree
Reading state information... Done
The following additional packages will be installed:
  libpq5
The following NEW packages will be installed:
  libpq5 sysbench
0 upgraded, 2 newly installed, 0 to remove and 55 not upgraded.
Need to get 444 kB of archives.
After this operation, 1469 kB of additional disk space will be used.
Get:1 http://mirrors.cloud.aliyuncs.com/ubuntu bionic-updates/main amd64 libpq5 amd64 10.12-0ubuntu0.18.04.1 [107 kB]
Get:2 https://packagecloud.io/akopytov/sysbench/ubuntu bionic/main amd64 sysbench amd64 1.0.20-1 [337 kB]
Fetched 444 kB in 8s (56.8 kB/s)
Selecting previously unselected package libpq5:amd64.
(Reading database ... 110345 files and directories currently installed.)
Preparing to unpack .../libpq5_10.12-0ubuntu0.18.04.1_amd64.deb ...
Unpacking libpq5:amd64 (10.12-0ubuntu0.18.04.1) ...
Selecting previously unselected package sysbench.
Preparing to unpack .../sysbench_1.0.20-1_amd64.deb ...
Unpacking sysbench (1.0.20-1) ...
Setting up libpq5:amd64 (10.12-0ubuntu0.18.04.1) ...
Setting up sysbench (1.0.20-1) ...
Processing triggers for libc-bin (2.27-3ubuntu1.2) ...
```
#### 2. 初始化 db
```shell
root@bench:~# mysql -uroot -h172.24.213.51 -P 4000
mysql> show global variables like '%txn%';
+-----------------------------------+-------------+
| Variable_name                     | Value       |
+-----------------------------------+-------------+
| tidb_disable_txn_auto_retry       | 1           |
| tidb_enable_amend_pessimistic_txn | on          |
| tidb_txn_mode                     | pessimistic |
+-----------------------------------+-------------+
3 rows in set (0.00 sec)

mysql> set global tidb_disable_txn_auto_retry=off;
Query OK, 0 rows affected (0.01 sec)
mysql> set global tidb_txn_mode="optimistic";
Query OK, 0 rows affected (0.01 sec)
```
```shell
root@bench:~# cat sysbench.conf
mysql-host=172.24.213.51
mysql-port=4000
mysql-user=root
mysql-password=
mysql-db=sbtest
time=600
threads=16
report-interval=10
db-driver=mysql

```
#### 3. 点查压测
```shell

```

