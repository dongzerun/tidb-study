Week 2 TiDB 基准测试
### 机器环境
阿里云机器，一共申请了 3 台 `ecs.c6.xlarge` 用于 PD, 3 台 `ecs.hfg6.2xlarge` 用于 tikv, 1 台 `ecs.c6.2xlarge` 用于 tidb, 1 台 `ecs.hfg6.2xlarge` 用于 benchmark client
|   | cpu  | memory  |  disk | ip  |
|---|---|---|---|---|
|  pd1 | 4  |  8G |  20G | 172.24.213.46  |
| pd2  |  4 |  8G | 20G  | 172.24.213.45  |
|  pd3 |  4 |  8G | 20G  | 172.24.213.47  |
| tikv1  |  8 |  32G | 100G SSD |  172.24.213.50 |
|  tikv1 | 8  | 32G  |  100G SSD | 172.24.213.48  |
|  tikv1 |  8 | 32G  | 100G SSD  | 172.24.213.49  |
|  tidb |  8 | 16G  | 20G  | 172.24.213.51  |
|  bench |   |  |   |   |
### TiKV 挂载 SSD
阿里云申请了各 100G 的 SSD, 需要格式化成 ext4 并挂载，第一步格式化文件系统
```shell
root@tikv001:~# fdisk -l
Disk /dev/vda: 20 GiB, 21474836480 bytes, 41943040 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0xc0f4f2f1

Device     Boot Start      End  Sectors Size Id Type
/dev/vda1  *     2048 41940991 41938944  20G 83 Linux


Disk /dev/vdb: 100 GiB, 107374182400 bytes, 209715200 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
root@tikv001:~#
root@tikv001:~#
root@tikv001:~# parted -s -a optimal /dev/vdb  mklabel gpt -- mkpart primary ext4 1 -1
root@tikv001:~# mkfs.ext4 /dev/vdb
mke2fs 1.44.1 (24-Mar-2018)
Found a gpt partition table in /dev/vdb
Proceed anyway? (y,N) y
Creating filesystem with 26214400 4k blocks and 6553600 inodes
Filesystem UUID: d7f9666d-4e47-4c2d-b9c2-d44cbc34be75
Superblock backups stored on blocks:
    32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208,
    4096000, 7962624, 11239424, 20480000, 23887872

Allocating group tables: done
Writing inode tables: done
Creating journal (131072 blocks): done
Writing superblocks and filesystem accounting information: done
```
第二步，挂载到 /tidb-data, 三台 tikv 机器重复这两步
```shell
root@tikv001:~# lsblk -f
NAME   FSTYPE LABEL UUID                                 MOUNTPOINT
vda
└─vda1 ext4   /     3d375280-1ab5-4559-a895-845bc4cb23c0 /
vdb    ext4         d7f9666d-4e47-4c2d-b9c2-d44cbc34be7
root@tikv001:~# echo "UUID=d7f9666d-4e47-4c2d-b9c2-d44cbc34be75 /tidb-data ext4 defaults,nodelalloc,noatime 0 2" >> /etc/fstab
root@tikv001:~# cat /etc/fstab
# /etc/fstab: static file system information.
#
# Use 'blkid' to print the universally unique identifier for a
# device; this may be used with UUID= as a more robust way to name devices
# that works even if disks are added and removed. See fstab(5).
#
# <file system> <mount point>   <type>  <options>       <dump>  <pass>
# / was on /dev/vda1 during installation
UUID=3d375280-1ab5-4559-a895-845bc4cb23c0 /               ext4    errors=remount-ro 0       1
/dev/fd0        /media/floppy0  auto    rw,user,noauto,exec,utf8 0       0
UUID=d7f9666d-4e47-4c2d-b9c2-d44cbc34be75 /tidb-data ext4 defaults,nodelalloc,noatime 0 2
root@tikv001:~# mkdir /tidb-data
root@tikv001:~# mount -a
```
查看挂载情况
```shell
root@tikv001:~# mount -t ext4
/dev/vda1 on / type ext4 (rw,relatime,errors=remount-ro,data=ordered)
/dev/vdb on /tidb-data type ext4 (rw,noatime,nodelalloc,data=ordered)
root@tikv001:~# df -h
Filesystem      Size  Used Avail Use% Mounted on
udev             16G     0   16G   0% /dev
tmpfs           3.1G  4.0M  3.1G   1% /run
/dev/vda1        20G  3.2G   16G  17% /
tmpfs            16G     0   16G   0% /dev/shm
tmpfs           5.0M     0  5.0M   0% /run/lock
tmpfs            16G     0   16G   0% /sys/fs/cgroup
tmpfs           3.1G     0  3.1G   0% /run/user/0
/dev/vdb         98G   61M   93G   1% /tidb-data
```
### Tiup 下载安装
```shell
root@tidb:~# curl --proto '=https' --tlsv1.2 -sSf https://tiup-mirrors.pingcap.com/install.sh | sh
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 4422k  100 4422k    0     0  4735k      0 --:--:-- --:--:-- --:--:-- 4730k
WARN: adding root certificate via internet: https://tiup-mirrors.pingcap.com/root.json
You can revoke this by remove /root/.tiup/bin/7b8e153f2e2d0928.root.json
Set mirror to https://tiup-mirrors.pingcap.com success
Detected shell: bash
Shell profile:  /root/.bashrc
/root/.bashrc has been modified to to add tiup to PATH
open a new terminal or source /root/.bashrc to use it
Installed path: /root/.tiup/bin/tiup
===============================================
Have a try:     tiup playground
===============================================
root@tidb:~# source /root/.bashrc
```
### Tiup 启动集群
从官网下载好 `topology.yaml` 模版，编辑填写正确的 ip 地址
```shell
root@tidb:~# tiup cluster deploy tidb-benchmark nightly ./topology.yaml --user root -p
The component `cluster` is not installed; downloading from repository.
download https://tiup-mirrors.pingcap.com/cluster-v1.0.9-linux-amd64.tar.gz 9.86 MiB / 9.86 MiB 100.00% ? p/s
Starting component `cluster`: /root/.tiup/components/cluster/v1.0.9/tiup-cluster deploy tidb-benchmark nightly ./topology.yaml --user root -p
Please confirm your topology:
tidb Cluster: tidb-benchmark
tidb Version: nightly
Type          Host           Ports        OS/Arch       Directories
----          ----           -----        -------       -----------
pd            172.24.213.46  2379/2380    linux/x86_64  /tidb-deploy/pd-2379,/tidb-data/pd-2379
pd            172.24.213.45  2379/2380    linux/x86_64  /tidb-deploy/pd-2379,/tidb-data/pd-2379
pd            172.24.213.47  2379/2380    linux/x86_64  /tidb-deploy/pd-2379,/tidb-data/pd-2379
tikv          172.24.213.50  20160/20180  linux/x86_64  /tidb-deploy/tikv-20160,/tidb-data/tikv-20160
tikv          172.24.213.48  20160/20180  linux/x86_64  /tidb-deploy/tikv-20160,/tidb-data/tikv-20160
tikv          172.24.213.49  20160/20180  linux/x86_64  /tidb-deploy/tikv-20160,/tidb-data/tikv-20160
tidb          172.24.213.51  4000/10080   linux/x86_64  /tidb-deploy/tidb-4000
prometheus    172.24.213.49  9090         linux/x86_64  /tidb-deploy/prometheus-9090,/tidb-data/prometheus-9090
grafana       172.24.213.48  3000         linux/x86_64  /tidb-deploy/grafana-3000
alertmanager  172.24.213.50  9093/9094    linux/x86_64  /tidb-deploy/alertmanager-9093,/tidb-data/alertmanager-9093
Attention:
    1. If the topology is not what you expected, check your yaml file.
    2. Please confirm there is no port/directory conflicts in same host.
Do you want to continue? [y/N]:  y
```
